# Configurazione SAC
policy: "MlpPolicy"
ent_coef: "auto" 
target_update_interval: 1   
target_entropy: "auto"      
gamma: 0.98                
tau: 0.02                 
seed: 42                   
device: "auto" 
action_noise: null
use_sde: True   
policy_kwargs:
  log_std_init: -2.0
  net_arch: 
    pi: [256, 128, 128]
    qf: [256, 128, 128]

# Configurazione addestramento
learning_rate: "constant_0.00073"
learning_starts: 10_000  # quando iniziare apprendere la policy   
n_timesteps: 1_000_000 # numero totale di timesteps per l'addestramento
train_freq: 64
gradient_steps: 64

# Normalizzazioni
normalize_input: True
normalize_value: False # da rivere a seconda della variabilit√† delle ricompense 
batch_size: 1024 # default 512         
buffer_size: 2_000_000 
clip_obs: 10.0         